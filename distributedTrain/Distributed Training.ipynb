{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPSL8usOE55n"
   },
   "source": [
    "# Why Distributed Training?\n",
    "The process of deep learning is very time consuming as there is a need to process huge amounts of data. One has to speed up the training process in order to obtain faster results. Distributed training is one such technology which helps reduce training time. It is based on the idea of training the model using different data in parallel on different machines instead of doing it serially on one machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhwrbwe93X_n"
   },
   "source": [
    "# Distributed training with TensorFlow\n",
    "`tf.distribute.Strategy` is a TensorFlow API used to distribute training across multiple GPUs, multiple machines or TPUs.\\\n",
    "`tf.distribute.Strategy` can be used with high level API like Keras, and can also be used to define custom training loops (in general, any computation using TensorFlow).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eRyQZu63isd"
   },
   "source": [
    "## Types of strategies\n",
    "`tf.distribute.Strategy` covers a number of use cases along different axes. Some of these are:\n",
    "\n",
    "\n",
    "*   *Synchronous vs asynchronous training:* In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Sync training is supported via all-reduce and async through parameter server architecture.\n",
    "\n",
    "*   *Hardware platform:* Scaling the training onto multiple GPUs on one machine, or multiple machines in a network (with none or some GPUs) or on Cloud TPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TwzfMhJAk11"
   },
   "source": [
    "### Parameter Server\n",
    "Their role is to store the parameters of a machine learning model like weights and serve them to clients which process the data and update these parameters.\\\n",
    "Many machine learning problems rely on large number of data for training and inference. In such big models, the learning and inference in a single machine is not possible. It would be helpful to have a framework that can be used for distributed learning as well as inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVv6nMQeAsmr"
   },
   "source": [
    "### AllReduce\n",
    "An operation that reduces the target arrays in all processes to a single array and returns the resultant array to all the processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gqpTKKOHH46"
   },
   "source": [
    "## Scenarios\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D92hS6SwJu0G"
   },
   "source": [
    "### Mirrored Strategy\n",
    "`tf.distribute.MirroredStrategy` supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. Together they form a single `MirroredVariable`. These variables are kept in sync with each other by applying identical updates.\\\n",
    "Efficient all-reduce algorithms are used to communicate the variable updates across the devices. All-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zboNqmGuJxE0"
   },
   "source": [
    "### Central Strategy\n",
    "`tf.distribute.experimental.CentralStorageStrategy` does synchronous training as well. Variables are not mirrored, instead placed on the CPU and operations are replicated across all local GPUs. Update to variables on replicas will be aggregated before being applied to variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xuSFU-iqKfim"
   },
   "source": [
    "### MultiWorkerMirroredStrategy\n",
    "`tf.distribute.experimental.MultiWorkerMirroredStrategy` is very similar to `MirroredStrategy`. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to `MirroredStrategy`, it creates copies of all variables in the model on each device across all workers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaTPG4RVSMQZ"
   },
   "source": [
    "### TPUStrategy\n",
    "`tf.distribute.experimental.TPUStrategy` lets one to run TensorFlow training on Tensor Processing Units (TPUs). TPUs are Google's specialized application specific IC designed to accelerate machine learning workloads.\\\n",
    "In terms of distributed training architecture, `TPUStrategy` is the same as `MirrorStrategy` - it implements synchronous distributed training. TPUs provide their own implementation of efficient all-reduce and other collective operations across multiple TPU cores, which are used in `TPUStrategy`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DosmdXrNTkJN"
   },
   "source": [
    "### ParameterServerStrategy\n",
    "`tf.distribute.experimental.ParameterServerStrategy` supports parameter training on multiple machines. In this setup, some machines are designated as workers and some as parameter servers. Each variable of the model is placed on one paramter server. Computation is replicated across all GPUs of workers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P38PDLw5ihf8"
   },
   "source": [
    "### OneDeviceStrategy\n",
    "`tf.distribute.OneDeviceStrategy` runs on a single device. This will place all the variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specific device.\\\n",
    "One could use this strategy to test the code before switching to other strategies which distributes to multiple machines/devices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRC79xq-vSLm"
   },
   "source": [
    "# Defining Loss\n",
    "On a single machine, with 1 GPU/CPU, loss is divided by the number of examples in the input batch. But with distributed training, there are many such batches running parallelly.\\\n",
    "Calculating loss using `tf.distribute.Strategy`:\n",
    "\n",
    "\n",
    "*   Say there are 4 GPUs and batch size of 64. One batch is distributed across the replicas (4 GPUs), with each replica getting an input of size 16.\n",
    "*   The model on each replica does a forward pass with the respective input and calculates loss. Instead of dividing the loss by number of examples in respective input (BATCH_SIZE_PER_REPLICA=16), it should be divided by the total number of examples in the batch (GLOBAL_BATCH_SIZE=64).\\\n",
    "This is done because after each replica, the gradients are synced across the replicas by summing them.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odB3FGOqxI-x"
   },
   "source": [
    "## Procedure in TensorFlow\n",
    "\n",
    "\n",
    "*   For a custom training loop, the loss per example must be summed and then divided by the GLOABL_BATCH_SIZE: `scale_loss = tf.reduce_sum(loss)*(1./GLOBAL_BATCH_SIZE)` or `tf.nn.compute_average_loss` with the per example loss, optional sample weights and GLOBAL_BATCH_SIZE  as arguements and returns the scaled loss.\n",
    "*   If using regularization losses in the model then the loss value has to be scaled by the number of replicas. This could be done by using the `tf.nn.scale_regularization_loss` function.\n",
    "* Using `tf.mean` is not recommended as this divides the loss by BATCH_SIZE_PER_REPLICA which may vary for each step.\n",
    "* In keras `model.compile` and `model.fit`, the reduction and scaling is automatically done.\n",
    "* If using `tf.keras.losses` classes, the loss reduction needs to be explicitly specified to `NONE` or `SUM`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBvLzfo1nan3"
   },
   "source": [
    "# References\n",
    "\n",
    "1. https://www.tensorflow.org/guide/distributed_training\n",
    "2.   https://lambdalabs.com/blog/introduction-multi-gpu-multi-node-distributed-training-nccl-2-0/\n",
    "3.  https://missinglink.ai/guides/tensorflow/tensorflow-distributed-training-introduction-tutorials/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3OnWW8m2fFH"
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFIu0kFm2iJR"
   },
   "source": [
    "## Custom training with tf.distribute.Strategy\n",
    "The tutorial demonstrates using of `tf.distribute.Strategy` with custom training loops. A simple CNN model on fashion MNIST dataset is trained.  The fashion MNIST dataset contains 60000 train images of size 28 x 28 and 10000 test images of size 28 x 28.\\\n",
    "\n",
    "Custom training loops are used to train the model because they give flexibility and a greater control on training. Moreover, it is easier to debug the model and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBolPN_n2rfZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWLCiIHl2u_1"
   },
   "source": [
    "### Download the fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46fWxcgy3RDU"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Adding a dimension to the array -> new shape == (28, 28, 1)\n",
    "# We are doing this because the first layer in our model is a convolutional\n",
    "# layer and it requires a 4D input (batch_size, height, width, channels).\n",
    "# batch_size dimension will be added later on.\n",
    "train_images = train_images[..., None]\n",
    "test_images = test_images[..., None]\n",
    "\n",
    "# Getting the images in [0, 1] range.\n",
    "train_images = train_images / np.float32(255)\n",
    "test_images = test_images / np.float32(255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxf_olrW3TBO"
   },
   "source": [
    "### Create a strategy to distribute the variables and the graph\n",
    "\n",
    "Working of `tf.distribute.MirroredStrategy`:\n",
    "\n",
    "*   All variables and model graph is replicated on the replicas\n",
    "*   Input is evenly distributed on the replicas \n",
    "* Each replica calculates the loss and gradients for the input it received\n",
    "* The gradients are synced across all replicas by summing them\n",
    "* After sync, the same update is made to the copies of the variables on each replica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbcZw-XZ3Wxo"
   },
   "outputs": [],
   "source": [
    "# If the list of devices is not specified in the\n",
    "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
    "strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wHP-oA95Qyo"
   },
   "outputs": [],
   "source": [
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytLifXZE5U0m"
   },
   "source": [
    "### Setting up input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NV4JNI-I5iOa"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_images)\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cTT_a705kOw"
   },
   "source": [
    "Create the datasets and distribute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpXGLioL5pff"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE) \n",
    "\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJn6lq1j6l05"
   },
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4iQRLu86toV"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iulb9ZLW6uzO"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TI-rUxnK6xcf"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56SycSW76zrn"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
    "  # global batch size.\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      reduction=tf.keras.losses.Reduction.NONE)\n",
    "  # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "  def compute_loss(labels, predictions):\n",
    "    per_example_loss = loss_object(labels, predictions)\n",
    "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8SlmKXi63D6"
   },
   "source": [
    "### Metrics to track loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6MlqDmo68XU"
   },
   "source": [
    "These metrics track the test loss and training and test accuracy. The `.result()` can be used to get the accumulated statistics anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcFEv5xy66do"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='test_accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkM6Va9q7Jxd"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbjORl-g7MV1"
   },
   "outputs": [],
   "source": [
    "# model and optimizer must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "  model = create_model()\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4d56ViYd7N5F"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  def train_step(inputs):\n",
    "    images, labels = inputs\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(images, training=True)\n",
    "      loss = compute_loss(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss \n",
    "\n",
    "  def test_step(inputs):\n",
    "    images, labels = inputs\n",
    "\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss.update_state(t_loss)\n",
    "    test_accuracy.update_state(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C09-yTqr7TS0"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # `experimental_run_v2` replicates the provided computation and runs it\n",
    "  # with the distributed input.\n",
    "  @tf.function\n",
    "  def distributed_train_step(dataset_inputs):\n",
    "    per_replica_losses = strategy.experimental_run_v2(train_step,\n",
    "                                                      args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                           axis=None)\n",
    " \n",
    "  @tf.function\n",
    "  def distributed_test_step(dataset_inputs):\n",
    "    return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    # TRAIN LOOP\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in train_dist_dataset:\n",
    "      total_loss += distributed_train_step(x)\n",
    "      num_batches += 1\n",
    "    train_loss = total_loss / num_batches\n",
    "\n",
    "    # TEST LOOP\n",
    "    for x in test_dist_dataset:\n",
    "      distributed_test_step(x)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "      checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "                \"Test Accuracy: {}\")\n",
    "    print (template.format(epoch+1, train_loss,\n",
    "                           train_accuracy.result()*100, test_loss.result(),\n",
    "                           test_accuracy.result()*100))\n",
    "\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTbNrepH-6k7"
   },
   "source": [
    "### Restoring the latest checkpoint and test\n",
    "A model checkpointed with `tf.distribute.Strategy` can be restored with or without strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJFsfWg2_LOe"
   },
   "outputs": [],
   "source": [
    "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='eval_accuracy')\n",
    "\n",
    "new_model = create_model()\n",
    "new_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJwpt64d_MV9"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(images, labels):\n",
    "  predictions = new_model(images, training=False)\n",
    "  eval_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUzO-S5D_Noz"
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "  eval_step(images, labels)\n",
    "\n",
    "print ('Accuracy after restoring the saved model without strategy: {}'.format(\n",
    "    eval_accuracy.result()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rsBqa2RaBz2L"
   },
   "source": [
    "## Distributed training with Keras\n",
    "This example uses `tf.keras` API to build the model and training loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwLUxHk_CZkB"
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuuPk1UBCcVD"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Import TensorFlow and TensorFlow Datasets\n",
    "try:\n",
    "  !pip install -q tf-nightly\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSyh1M3fCeep"
   },
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QNSlXVcCk48"
   },
   "source": [
    "Setting `with_info` to `True` includes the metadata for the entire dataset, which is being saved here in `info`. The metadata object includes the number of train and test examples and other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XhdnBDSChG4"
   },
   "outputs": [],
   "source": [
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdlnQb2pC89t"
   },
   "source": [
    "### Define distribution strategy\n",
    "Create a `MirroredStrategy` object. This will handle distribution, and provides a context manager (`tf.distribute.MirroredStrategy.scope`) to build the model inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HaFnlxH6DH4i"
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDFeMuc_DKN_"
   },
   "source": [
    "### Setup the input pipeline\n",
    "When training the model with multiple GPUs, the extra computing power can be used by effectively increasing the batch size. The largest batch size that could fit in the GPU memory could be used and the learning rate be tuned accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLGYrcQADh9x"
   },
   "outputs": [],
   "source": [
    "# You can also do info.splits.total_num_examples to get the total\n",
    "# number of examples in the dataset.\n",
    "\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rm71p4kiDivU"
   },
   "source": [
    "Pixel values, which are 0-255, have to be normalized to the 0-1 range. Define this scale in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lnz5i3kXDlkj"
   },
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrw6C9T-Do2X"
   },
   "source": [
    "Apply this function to the training and test data, shuffle the training data, and batch it for training. Notice we are also keeping an in-memory cache of the training data to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2iSx8z7ADpbu"
   },
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2bt-UGvDrYb"
   },
   "source": [
    "### Model creation\n",
    "Create and compile Keras model in `strategy.scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TvnIRcUD0L3"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GbhjVDgD2QV"
   },
   "source": [
    "### Defining callbacks\n",
    "The callbacks used here are:\n",
    "* *TensorBoard*: This callback writes a log for TensorBoard which allows the visualization of the graphs.\n",
    "* *Model Checkpoint*: This callback saves the model after every epoch.\n",
    "* *Learning Rate Scheduler*: Using this callback, the learning rate can be scheduled to change after every epoch/batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hhGHHxXENtn"
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVkMWYRdEOe8"
   },
   "outputs": [],
   "source": [
    "# Function for decaying the learning rate.\n",
    "# You can define any decay function you need.\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3V9ZaACEQkP"
   },
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model.optimizer.lr.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfzNepdTERn2"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfxOOWzLEUrl"
   },
   "source": [
    "### Train and evaluate\n",
    "The model is trained using `model.fit` the usual way by passing in the dataset created at the beginning. This step is the same whether the training is distributed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qz8Um6ewEWuh"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=12, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qqy_AGTFEyX3"
   },
   "source": [
    "To see how the model perform, load the latest checkpoint and call `evaluate` on the test data.\n",
    "\n",
    "Call `evaluate` as before using appropriate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zzUKHkxE6nu"
   },
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpinnFYhE_35"
   },
   "source": [
    "To see the output, the TensorBoard logs can be downloaded and viewed at the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDSsNbloFIA-"
   },
   "outputs": [],
   "source": [
    "$ tensorboard --logdir=path/to/log-directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tbt1dnbqFImt"
   },
   "outputs": [],
   "source": [
    "!ls -sh ./logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mc7PA42CFKaa"
   },
   "source": [
    "### Export to Saved Model\n",
    "Export the graph and the variables to the platform-agnostic SavedModel format. After the model is saved, it can be loaded with or without the scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8H1mBMiuFPqF"
   },
   "outputs": [],
   "source": [
    "path = 'saved_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foxFkpHuFVoW"
   },
   "outputs": [],
   "source": [
    "model.save(path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htwpZmxmFYGc"
   },
   "source": [
    "Load the model without `strategy.scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN40zHZKFXwX"
   },
   "outputs": [],
   "source": [
    "unreplicated_model = tf.keras.models.load_model(path)\n",
    "\n",
    "unreplicated_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvT9V4aUFfSY"
   },
   "source": [
    "Load the model with `strategy.scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfwuIovVFiQc"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  replicated_model = tf.keras.models.load_model(path)\n",
    "  replicated_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                           optimizer=tf.keras.optimizers.Adam(),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n",
    "  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yWLCiIHl2u_1",
    "vxf_olrW3TBO",
    "ytLifXZE5U0m",
    "gJn6lq1j6l05",
    "TI-rUxnK6xcf",
    "z8SlmKXi63D6",
    "RkM6Va9q7Jxd",
    "ZTbNrepH-6k7",
    "NwLUxHk_CZkB",
    "fSyh1M3fCeep",
    "DdlnQb2pC89t",
    "SDFeMuc_DKN_",
    "V2bt-UGvDrYb",
    "-GbhjVDgD2QV",
    "WfxOOWzLEUrl",
    "Mc7PA42CFKaa"
   ],
   "name": "Distributed Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_metadata": {
   "author": "Ajay N R",
   "title": "A brief report on Distributed Training"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
